{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:26.218025Z",
     "start_time": "2017-10-16T03:29:18.836212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from RubiksEnv import RubiksEnv\n",
    "\n",
    "from numba import jit, uint8, float32\n",
    "\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.agents import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:26.739827Z",
     "start_time": "2017-10-16T03:29:26.218965Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(float32(uint8[:, :, :, :]),nopython=True)\n",
    "def rubiks_hamilton_dist(cube):\n",
    "    x, y, z = cube.shape[:-1]\n",
    "    distance_corner = float(0.0)\n",
    "    distance_edge = float(0.0)\n",
    "    \n",
    "    distance_rot = float(0.0)\n",
    "    \n",
    "    \n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            for k in range(z):\n",
    "                cubie = cube[i,j,k]\n",
    "                dist_rot = float(0.0)\n",
    "                \n",
    "                if np.any(cubie == 5):\n",
    "                    i_true = 0\n",
    "                    dist_rot += cubie[0] != 5\n",
    "                elif np.any(cubie == 3):\n",
    "                    i_true = 2\n",
    "                    dist_rot += cubie[0] != 3\n",
    "                else:\n",
    "                    i_true = 1\n",
    "                    \n",
    "                if np.any(cubie == 4):\n",
    "                    j_true = 0\n",
    "                    dist_rot += cubie[1] != 4\n",
    "                elif np.any(cubie == 2):\n",
    "                    j_true = 2\n",
    "                    dist_rot += cubie[1] != 2\n",
    "                else:\n",
    "                    j_true = 1\n",
    "                    \n",
    "                if np.any(cubie == 1):\n",
    "                    k_true = 0\n",
    "                    dist_rot += cubie[2] != 1\n",
    "                elif np.any(cubie == 6):\n",
    "                    k_true = 2\n",
    "                    dist_rot += cubie[2] != 6\n",
    "                else:\n",
    "                    k_true = 1\n",
    "                \n",
    "                distance = abs(i - i_true) + abs(j - j_true) + abs(k - k_true)\n",
    "                \n",
    "                num_zero = (cubie == 0).sum()\n",
    "                if (num_zero == 0):\n",
    "                    distance_corner += distance\n",
    "                elif (num_zero == 1):\n",
    "                    distance_edge += distance\n",
    "                    \n",
    "                distance_rot += dist_rot / float(3.0)\n",
    "                    \n",
    "    return 1 / (max(distance_corner, distance_edge) + distance_rot + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:26.743117Z",
     "start_time": "2017-10-16T03:29:26.740960Z"
    }
   },
   "outputs": [],
   "source": [
    "DIFFICULTY = 4\n",
    "env = RubiksEnv(rubiks_hamilton_dist, scrambles=DIFFICULTY, max_step=DIFFICULTY)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:26.748268Z",
     "start_time": "2017-10-16T03:29:26.744112Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = env.observation_space.shape\n",
    "WINDOW_LENGTH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:26.791096Z",
     "start_time": "2017-10-16T03:29:26.749296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 3, 3, 3, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 3, 3, 3, 8)        32        \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 1, 1, 1, 64)       13888     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 18,860\n",
      "Trainable params: 18,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Reshape(INPUT_SHAPE, input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE))\n",
    "\n",
    "# Mutli Conv\n",
    "# model.add(keras.layers.Convolution3D(filters=8, kernel_size=(1, 1, 1), strides=(1, 1, 1), activation='elu'))\n",
    "# model.add(keras.layers.Convolution3D(filters=16, kernel_size=(1, 1, 3), strides=(1, 1, 1), padding='same', activation='elu'))\n",
    "# model.add(keras.layers.Convolution3D(filters=16, kernel_size=(1, 3, 1), strides=(1, 1, 1), padding='same', activation='elu'))\n",
    "# model.add(keras.layers.Convolution3D(filters=16, kernel_size=(3, 1, 1), strides=(1, 1, 1), padding='same', activation='elu'))\n",
    "# model.add(keras.layers.Convolution3D(filters=16, kernel_size=(1, 1, 1), strides=(1, 1, 1), activation='elu'))\n",
    "# model.add(keras.layers.Flatten())\n",
    "\n",
    "# model.add(keras.layers.Dense(32, activation='elu'))\n",
    "# model.add(keras.layers.Dense(16, activation='elu'))\n",
    "# model.add(keras.layers.Dense(nb_actions, activation='linear'))\n",
    "\n",
    "model.add(layers.Conv3D(8, kernel_size=(1, 1, 1), strides=(1, 1, 1), activation='elu'))\n",
    "model.add(layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), activation='elu'))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='elu'))\n",
    "model.add(layers.Dense(nb_actions, activation='linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:27.612188Z",
     "start_time": "2017-10-16T03:29:27.583559Z"
    }
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=WINDOW_LENGTH)\n",
    "processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:28.107085Z",
     "start_time": "2017-10-16T03:29:28.102438Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=0.9, value_min=.05, value_test=.05, nb_steps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:28.560456Z",
     "start_time": "2017-10-16T03:29:28.533706Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, nb_steps_warmup=50000, gamma=.99, target_model_update=0.8, train_interval=4, enable_dueling_network=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:29:29.388565Z",
     "start_time": "2017-10-16T03:29:28.997361Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.compile(keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T03:54:14.372958Z",
     "start_time": "2017-10-16T03:29:29.949350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 16s - reward: 0.0378    - ETA: 0\n",
      "2516 episodes - episode_reward: 0.150 [0.091, 1.199]\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 15s - reward: 0.0375    \n",
      "2513 episodes - episode_reward: 0.149 [0.088, 1.199]\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 15s - reward: 0.0381    \n",
      "2516 episodes - episode_reward: 0.152 [0.091, 1.213]\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 15s - reward: 0.0381    \n",
      "2515 episodes - episode_reward: 0.152 [0.090, 1.199]\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 15s - reward: 0.0382    - ET\n",
      "2518 episodes - episode_reward: 0.152 [0.085, 1.199]\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0547    \n",
      "2576 episodes - episode_reward: 0.212 [0.091, 1.213] - loss: 0.189 - mean_q: 1.604 - mean_eps: 0.807\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.0556    \n",
      "2570 episodes - episode_reward: 0.216 [0.093, 1.213] - loss: 0.186 - mean_q: 1.622 - mean_eps: 0.790\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.0496    \n",
      "2542 episodes - episode_reward: 0.195 [0.090, 1.213] - loss: 0.122 - mean_q: 1.318 - mean_eps: 0.773\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0501    - ETA: 0s\n",
      "2544 episodes - episode_reward: 0.197 [0.085, 1.213] - loss: 0.078 - mean_q: 1.072 - mean_eps: 0.756\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.0518    \n",
      "2546 episodes - episode_reward: 0.204 [0.092, 1.213] - loss: 0.065 - mean_q: 0.983 - mean_eps: 0.739\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.0531    \n",
      "2550 episodes - episode_reward: 0.208 [0.091, 1.213] - loss: 0.063 - mean_q: 0.968 - mean_eps: 0.722\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0587    \n",
      "2575 episodes - episode_reward: 0.228 [0.090, 1.213] - loss: 0.055 - mean_q: 0.908 - mean_eps: 0.705\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.0659    \n",
      "2586 episodes - episode_reward: 0.255 [0.091, 1.213] - loss: 0.054 - mean_q: 0.908 - mean_eps: 0.688\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0768    \n",
      "2629 episodes - episode_reward: 0.292 [0.090, 1.213] - loss: 0.048 - mean_q: 0.857 - mean_eps: 0.671\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.0826    \n",
      "2648 episodes - episode_reward: 0.312 [0.088, 1.213] - loss: 0.046 - mean_q: 0.857 - mean_eps: 0.654\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.0921    - ETA: 0s - rewa - ETA: \n",
      "2692 episodes - episode_reward: 0.342 [0.093, 1.213] - loss: 0.046 - mean_q: 0.859 - mean_eps: 0.637\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0914    - ETA: 0s - rewar\n",
      "2675 episodes - episode_reward: 0.342 [0.091, 1.213] - loss: 0.045 - mean_q: 0.861 - mean_eps: 0.620\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.0984    \n",
      "2707 episodes - episode_reward: 0.364 [0.092, 1.213] - loss: 0.045 - mean_q: 0.865 - mean_eps: 0.603\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.1157    \n",
      "2766 episodes - episode_reward: 0.418 [0.085, 1.213] - loss: 0.044 - mean_q: 0.858 - mean_eps: 0.586\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.1221    \n",
      "2774 episodes - episode_reward: 0.440 [0.096, 1.213] - loss: 0.044 - mean_q: 0.870 - mean_eps: 0.569\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 28s - reward: 0.1216    \n",
      "2777 episodes - episode_reward: 0.438 [0.090, 1.213] - loss: 0.043 - mean_q: 0.863 - mean_eps: 0.552\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.1289    \n",
      "2798 episodes - episode_reward: 0.461 [0.088, 1.213] - loss: 0.044 - mean_q: 0.883 - mean_eps: 0.535\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.1416    \n",
      "2857 episodes - episode_reward: 0.496 [0.094, 1.213] - loss: 0.043 - mean_q: 0.885 - mean_eps: 0.518\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.1475    \n",
      "2869 episodes - episode_reward: 0.514 [0.092, 1.213] - loss: 0.043 - mean_q: 0.892 - mean_eps: 0.501\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.1486    \n",
      "2873 episodes - episode_reward: 0.517 [0.093, 1.213] - loss: 0.043 - mean_q: 0.898 - mean_eps: 0.484\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.1627    \n",
      "2917 episodes - episode_reward: 0.558 [0.091, 1.213] - loss: 0.042 - mean_q: 0.902 - mean_eps: 0.467\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.1779    \n",
      "2983 episodes - episode_reward: 0.596 [0.092, 1.213] - loss: 0.041 - mean_q: 0.905 - mean_eps: 0.450\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.1804    - ETA: 0s - rewa\n",
      "3000 episodes - episode_reward: 0.601 [0.092, 1.213] - loss: 0.042 - mean_q: 0.912 - mean_eps: 0.433\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 30s - reward: 0.1908    - ETA: 0s - reward: 0.190\n",
      "3024 episodes - episode_reward: 0.631 [0.095, 1.213] - loss: 0.040 - mean_q: 0.912 - mean_eps: 0.416\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.1899    \n",
      "3006 episodes - episode_reward: 0.632 [0.092, 1.213] - loss: 0.040 - mean_q: 0.926 - mean_eps: 0.399\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.2013    \n",
      "3050 episodes - episode_reward: 0.660 [0.096, 1.213] - loss: 0.040 - mean_q: 0.928 - mean_eps: 0.382\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.2142    \n",
      "3108 episodes - episode_reward: 0.689 [0.094, 1.213] - loss: 0.039 - mean_q: 0.929 - mean_eps: 0.365\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.2234    \n",
      "3138 episodes - episode_reward: 0.712 [0.091, 1.213] - loss: 0.039 - mean_q: 0.936 - mean_eps: 0.348\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.2442    \n",
      "3220 episodes - episode_reward: 0.758 [0.096, 1.213] - loss: 0.037 - mean_q: 0.939 - mean_eps: 0.331\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.2409    \n",
      "3216 episodes - episode_reward: 0.749 [0.095, 1.213] - loss: 0.036 - mean_q: 0.945 - mean_eps: 0.314\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.2578    \n",
      "3289 episodes - episode_reward: 0.784 [0.090, 1.213] - loss: 0.035 - mean_q: 0.944 - mean_eps: 0.297\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.2590    - ETA: 0s - rewar\n",
      "3307 episodes - episode_reward: 0.783 [0.088, 1.213] - loss: 0.034 - mean_q: 0.957 - mean_eps: 0.280\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.2726    \n",
      "3356 episodes - episode_reward: 0.812 [0.095, 1.213] - loss: 0.033 - mean_q: 0.959 - mean_eps: 0.263\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.2841    \n",
      "3408 episodes - episode_reward: 0.834 [0.092, 1.213] - loss: 0.032 - mean_q: 0.962 - mean_eps: 0.246\n",
      "\n",
      "Interval 40 (390000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 32s - reward: 0.3030    \n",
      "3520 episodes - episode_reward: 0.861 [0.095, 1.213] - loss: 0.031 - mean_q: 0.965 - mean_eps: 0.229\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.2929    \n",
      "3451 episodes - episode_reward: 0.849 [0.096, 1.213] - loss: 0.030 - mean_q: 0.976 - mean_eps: 0.212\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.3106    \n",
      "3543 episodes - episode_reward: 0.877 [0.096, 1.213] - loss: 0.029 - mean_q: 0.982 - mean_eps: 0.195\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.3137    - ETA: 0s - reward: 0.3\n",
      "3578 episodes - episode_reward: 0.877 [0.093, 1.213] - loss: 0.029 - mean_q: 0.986 - mean_eps: 0.178\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.3113    \n",
      "3523 episodes - episode_reward: 0.884 [0.103, 1.213] - loss: 0.030 - mean_q: 0.995 - mean_eps: 0.161\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.3309    \n",
      "3611 episodes - episode_reward: 0.916 [0.100, 1.213] - loss: 0.028 - mean_q: 0.994 - mean_eps: 0.144\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.3561    \n",
      "3748 episodes - episode_reward: 0.950 [0.100, 1.213] - loss: 0.027 - mean_q: 0.997 - mean_eps: 0.127\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.3578    \n",
      "3760 episodes - episode_reward: 0.952 [0.095, 1.213] - loss: 0.024 - mean_q: 1.000 - mean_eps: 0.110\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.3470    \n",
      "3690 episodes - episode_reward: 0.940 [0.098, 1.213] - loss: 0.023 - mean_q: 1.008 - mean_eps: 0.093\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.3377    \n",
      "3609 episodes - episode_reward: 0.936 [0.097, 1.213] - loss: 0.023 - mean_q: 1.023 - mean_eps: 0.076\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.3791    \n",
      "done, took 1484.409 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f095df575c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIFFICULTY = 2\n",
    "env = RubiksEnv(rubiks_hamilton_dist, scrambles=DIFFICULTY, max_step=DIFFICULTY)\n",
    "\n",
    "agent.fit(env, nb_steps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T21:33:03.496111Z",
     "start_time": "2017-10-15T21:02:44.100710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 26s - reward: 0.0802    \n",
      "2212 episodes - episode_reward: 0.363 [0.106, 1.257] - loss: 0.016 - mean_q: 0.995 - mean_eps: 0.592\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.0837    \n",
      "2216 episodes - episode_reward: 0.378 [0.098, 1.257] - loss: 0.017 - mean_q: 0.972 - mean_eps: 0.584\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0871    \n",
      "2236 episodes - episode_reward: 0.390 [0.096, 1.271] - loss: 0.020 - mean_q: 0.944 - mean_eps: 0.573\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0884    - ETA: 0s - reward:\n",
      "2210 episodes - episode_reward: 0.400 [0.103, 1.257] - loss: 0.023 - mean_q: 0.917 - mean_eps: 0.562\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0871    \n",
      "2228 episodes - episode_reward: 0.391 [0.095, 1.257] - loss: 0.026 - mean_q: 0.894 - mean_eps: 0.551\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.0927    \n",
      "2261 episodes - episode_reward: 0.410 [0.108, 1.257] - loss: 0.029 - mean_q: 0.867 - mean_eps: 0.540\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.0913    \n",
      "2234 episodes - episode_reward: 0.409 [0.102, 1.257] - loss: 0.033 - mean_q: 0.845 - mean_eps: 0.529\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.0940    \n",
      "2249 episodes - episode_reward: 0.418 [0.111, 1.271] - loss: 0.035 - mean_q: 0.816 - mean_eps: 0.518\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.0953    \n",
      "2262 episodes - episode_reward: 0.421 [0.105, 1.271] - loss: 0.037 - mean_q: 0.805 - mean_eps: 0.507\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1015    \n",
      "2262 episodes - episode_reward: 0.449 [0.108, 1.271] - loss: 0.038 - mean_q: 0.807 - mean_eps: 0.496\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1034    \n",
      "2275 episodes - episode_reward: 0.454 [0.104, 1.271] - loss: 0.039 - mean_q: 0.813 - mean_eps: 0.485\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1104    \n",
      "2290 episodes - episode_reward: 0.482 [0.107, 1.271] - loss: 0.039 - mean_q: 0.820 - mean_eps: 0.474\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1152    - ETA: 0s - r - ETA:  - ETA: 0s - re\n",
      "2321 episodes - episode_reward: 0.496 [0.110, 1.257] - loss: 0.040 - mean_q: 0.830 - mean_eps: 0.463\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1197    \n",
      "2342 episodes - episode_reward: 0.511 [0.107, 1.257] - loss: 0.039 - mean_q: 0.835 - mean_eps: 0.452\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1242    - ETA: 0s - reward: 0.12\n",
      "2358 episodes - episode_reward: 0.527 [0.108, 1.271] - loss: 0.040 - mean_q: 0.846 - mean_eps: 0.441\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1230    - ETA: 0s - reward: 0.12 - ETA: 0s - r\n",
      "2359 episodes - episode_reward: 0.522 [0.110, 1.257] - loss: 0.040 - mean_q: 0.846 - mean_eps: 0.430\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1335    \n",
      "2380 episodes - episode_reward: 0.561 [0.104, 1.271] - loss: 0.040 - mean_q: 0.852 - mean_eps: 0.419\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1354    \n",
      "2396 episodes - episode_reward: 0.565 [0.113, 1.271] - loss: 0.040 - mean_q: 0.856 - mean_eps: 0.408\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1369    \n",
      "2386 episodes - episode_reward: 0.574 [0.107, 1.271] - loss: 0.040 - mean_q: 0.862 - mean_eps: 0.397\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1406    \n",
      "2415 episodes - episode_reward: 0.582 [0.104, 1.271] - loss: 0.039 - mean_q: 0.863 - mean_eps: 0.386\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1484    \n",
      "2437 episodes - episode_reward: 0.609 [0.108, 1.257] - loss: 0.040 - mean_q: 0.869 - mean_eps: 0.375\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1462    \n",
      "2420 episodes - episode_reward: 0.604 [0.104, 1.257] - loss: 0.039 - mean_q: 0.876 - mean_eps: 0.364\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1558    - ETA: 0s - reward: 0.155\n",
      "2461 episodes - episode_reward: 0.633 [0.106, 1.257] - loss: 0.039 - mean_q: 0.877 - mean_eps: 0.353\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1593    \n",
      "2472 episodes - episode_reward: 0.644 [0.103, 1.257] - loss: 0.039 - mean_q: 0.882 - mean_eps: 0.342\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1596    \n",
      "2472 episodes - episode_reward: 0.646 [0.105, 1.271] - loss: 0.039 - mean_q: 0.886 - mean_eps: 0.331\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1703    \n",
      "2512 episodes - episode_reward: 0.678 [0.109, 1.257] - loss: 0.039 - mean_q: 0.894 - mean_eps: 0.320\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1656    \n",
      "2494 episodes - episode_reward: 0.664 [0.106, 1.271] - loss: 0.039 - mean_q: 0.894 - mean_eps: 0.309\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1668    \n",
      "2498 episodes - episode_reward: 0.668 [0.111, 1.257] - loss: 0.039 - mean_q: 0.900 - mean_eps: 0.298\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1784    \n",
      "2552 episodes - episode_reward: 0.699 [0.114, 1.271] - loss: 0.038 - mean_q: 0.905 - mean_eps: 0.287\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1770    \n",
      "2540 episodes - episode_reward: 0.697 [0.109, 1.271] - loss: 0.039 - mean_q: 0.912 - mean_eps: 0.276\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1903    \n",
      "2594 episodes - episode_reward: 0.734 [0.108, 1.257] - loss: 0.038 - mean_q: 0.915 - mean_eps: 0.265\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.2008    \n",
      "2654 episodes - episode_reward: 0.757 [0.104, 1.257] - loss: 0.037 - mean_q: 0.917 - mean_eps: 0.254\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1993    \n",
      "2628 episodes - episode_reward: 0.758 [0.111, 1.257] - loss: 0.037 - mean_q: 0.925 - mean_eps: 0.243\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.2076    \n",
      "2654 episodes - episode_reward: 0.782 [0.109, 1.271] - loss: 0.037 - mean_q: 0.928 - mean_eps: 0.232\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.1988    \n",
      "2662 episodes - episode_reward: 0.747 [0.106, 1.257] - loss: 0.036 - mean_q: 0.935 - mean_eps: 0.221\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.2031    - ETA: 0s - reward: 0.\n",
      "2670 episodes - episode_reward: 0.761 [0.110, 1.271] - loss: 0.036 - mean_q: 0.935 - mean_eps: 0.210\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.2110    \n",
      "2671 episodes - episode_reward: 0.790 [0.108, 1.271] - loss: 0.036 - mean_q: 0.940 - mean_eps: 0.199\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 37s - reward: 0.2226    \n",
      "2738 episodes - episode_reward: 0.813 [0.116, 1.271] - loss: 0.035 - mean_q: 0.943 - mean_eps: 0.188\n",
      "\n",
      "Interval 39 (380000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 34s - reward: 0.2072    \n",
      "2679 episodes - episode_reward: 0.773 [0.109, 1.271] - loss: 0.034 - mean_q: 0.950 - mean_eps: 0.177\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.2214    \n",
      "2742 episodes - episode_reward: 0.807 [0.111, 1.271] - loss: 0.034 - mean_q: 0.948 - mean_eps: 0.166\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.2276    - ETA: 0s - reward: 0.227\n",
      "2782 episodes - episode_reward: 0.818 [0.114, 1.271] - loss: 0.034 - mean_q: 0.949 - mean_eps: 0.155\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.2385    \n",
      "2791 episodes - episode_reward: 0.854 [0.111, 1.271] - loss: 0.034 - mean_q: 0.953 - mean_eps: 0.144\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.2432    \n",
      "2841 episodes - episode_reward: 0.856 [0.100, 1.271] - loss: 0.033 - mean_q: 0.960 - mean_eps: 0.133\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.2206    \n",
      "2753 episodes - episode_reward: 0.801 [0.108, 1.257] - loss: 0.033 - mean_q: 0.965 - mean_eps: 0.122\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.2460    \n",
      "2866 episodes - episode_reward: 0.858 [0.113, 1.257] - loss: 0.033 - mean_q: 0.967 - mean_eps: 0.111\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.2387    \n",
      "2836 episodes - episode_reward: 0.842 [0.112, 1.257] - loss: 0.033 - mean_q: 0.971 - mean_eps: 0.100\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.2560    \n",
      "2917 episodes - episode_reward: 0.878 [0.116, 1.271] - loss: 0.033 - mean_q: 0.973 - mean_eps: 0.089\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.2493    \n",
      "2898 episodes - episode_reward: 0.860 [0.112, 1.271] - loss: 0.032 - mean_q: 0.979 - mean_eps: 0.078\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.2585    \n",
      "2934 episodes - episode_reward: 0.881 [0.113, 1.271] - loss: 0.032 - mean_q: 0.978 - mean_eps: 0.067\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.2397    \n",
      "done, took 1819.391 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78906e11d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIFFICULTY = 3\n",
    "env = RubiksEnv(rubiks_hamilton_dist, scrambles=DIFFICULTY, max_step=DIFFICULTY)\n",
    "agent.policy.value_max = 0.6\n",
    "\n",
    "agent.fit(env, nb_steps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T22:02:51.229556Z",
     "start_time": "2017-10-15T21:33:03.496948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 27s - reward: 0.0475    \n",
      "1727 episodes - episode_reward: 0.275 [0.117, 1.331] - loss: 0.033 - mean_q: 0.961 - mean_eps: 0.592\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0543    \n",
      "1760 episodes - episode_reward: 0.308 [0.118, 1.348] - loss: 0.032 - mean_q: 0.939 - mean_eps: 0.584\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0512    - ETA: 0s - rewar\n",
      "1742 episodes - episode_reward: 0.294 [0.118, 1.330] - loss: 0.032 - mean_q: 0.915 - mean_eps: 0.573\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0547    \n",
      "1749 episodes - episode_reward: 0.313 [0.120, 1.320] - loss: 0.032 - mean_q: 0.887 - mean_eps: 0.562\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0565    - ETA: 0s - reward: 0.056\n",
      "1753 episodes - episode_reward: 0.322 [0.123, 1.348] - loss: 0.033 - mean_q: 0.858 - mean_eps: 0.551\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0607    \n",
      "1767 episodes - episode_reward: 0.343 [0.124, 1.320] - loss: 0.032 - mean_q: 0.825 - mean_eps: 0.540\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0602    \n",
      "1769 episodes - episode_reward: 0.340 [0.124, 1.334] - loss: 0.032 - mean_q: 0.804 - mean_eps: 0.529\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0605    - ETA: 0s - reward: 0.06\n",
      "1774 episodes - episode_reward: 0.341 [0.117, 1.348] - loss: 0.031 - mean_q: 0.776 - mean_eps: 0.518\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0635    \n",
      "1776 episodes - episode_reward: 0.358 [0.117, 1.334] - loss: 0.030 - mean_q: 0.735 - mean_eps: 0.507\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0713    \n",
      "1819 episodes - episode_reward: 0.392 [0.125, 1.331] - loss: 0.030 - mean_q: 0.730 - mean_eps: 0.496\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0729    \n",
      "1808 episodes - episode_reward: 0.403 [0.122, 1.348] - loss: 0.031 - mean_q: 0.749 - mean_eps: 0.485\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0706    \n",
      "1812 episodes - episode_reward: 0.390 [0.119, 1.320] - loss: 0.031 - mean_q: 0.750 - mean_eps: 0.474\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0754    \n",
      "1815 episodes - episode_reward: 0.415 [0.119, 1.334] - loss: 0.031 - mean_q: 0.751 - mean_eps: 0.463\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0782    \n",
      "1826 episodes - episode_reward: 0.428 [0.118, 1.334] - loss: 0.031 - mean_q: 0.758 - mean_eps: 0.452\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.0809    \n",
      "1833 episodes - episode_reward: 0.441 [0.116, 1.331] - loss: 0.031 - mean_q: 0.759 - mean_eps: 0.441\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0826    \n",
      "1835 episodes - episode_reward: 0.450 [0.123, 1.331] - loss: 0.032 - mean_q: 0.767 - mean_eps: 0.430\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0872    \n",
      "1848 episodes - episode_reward: 0.472 [0.122, 1.334] - loss: 0.032 - mean_q: 0.773 - mean_eps: 0.419\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0912    \n",
      "1865 episodes - episode_reward: 0.489 [0.119, 1.334] - loss: 0.032 - mean_q: 0.777 - mean_eps: 0.408\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0896    \n",
      "1851 episodes - episode_reward: 0.484 [0.123, 1.334] - loss: 0.032 - mean_q: 0.784 - mean_eps: 0.397\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0938    \n",
      "1874 episodes - episode_reward: 0.501 [0.124, 1.348] - loss: 0.031 - mean_q: 0.781 - mean_eps: 0.386\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0971    \n",
      "1889 episodes - episode_reward: 0.514 [0.125, 1.334] - loss: 0.031 - mean_q: 0.785 - mean_eps: 0.375\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0927    \n",
      "1870 episodes - episode_reward: 0.496 [0.121, 1.334] - loss: 0.032 - mean_q: 0.789 - mean_eps: 0.364\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.0990    \n",
      "1889 episodes - episode_reward: 0.524 [0.129, 1.334] - loss: 0.032 - mean_q: 0.795 - mean_eps: 0.353\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1002    \n",
      "1892 episodes - episode_reward: 0.529 [0.124, 1.331] - loss: 0.032 - mean_q: 0.804 - mean_eps: 0.342\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1104    - ETA: 0s - rewa\n",
      "1915 episodes - episode_reward: 0.577 [0.127, 1.334] - loss: 0.031 - mean_q: 0.800 - mean_eps: 0.331\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1108    \n",
      "1938 episodes - episode_reward: 0.572 [0.120, 1.331] - loss: 0.032 - mean_q: 0.811 - mean_eps: 0.320\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1113    \n",
      "1928 episodes - episode_reward: 0.577 [0.127, 1.334] - loss: 0.032 - mean_q: 0.810 - mean_eps: 0.309\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1136    - ETA: 0s - re\n",
      "1942 episodes - episode_reward: 0.585 [0.119, 1.334] - loss: 0.032 - mean_q: 0.816 - mean_eps: 0.298\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1178    - ETA: 0s - reward: 0.117\n",
      "1964 episodes - episode_reward: 0.600 [0.123, 1.348] - loss: 0.032 - mean_q: 0.820 - mean_eps: 0.287\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1247    \n",
      "1968 episodes - episode_reward: 0.634 [0.125, 1.334] - loss: 0.032 - mean_q: 0.830 - mean_eps: 0.276\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1281    \n",
      "2001 episodes - episode_reward: 0.640 [0.130, 1.331] - loss: 0.032 - mean_q: 0.835 - mean_eps: 0.265\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1232    \n",
      "1968 episodes - episode_reward: 0.626 [0.124, 1.331] - loss: 0.032 - mean_q: 0.837 - mean_eps: 0.254\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1279    \n",
      "1986 episodes - episode_reward: 0.644 [0.115, 1.334] - loss: 0.032 - mean_q: 0.847 - mean_eps: 0.243\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1287    \n",
      "1986 episodes - episode_reward: 0.648 [0.124, 1.334] - loss: 0.031 - mean_q: 0.848 - mean_eps: 0.232\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1289    \n",
      "1991 episodes - episode_reward: 0.647 [0.123, 1.331] - loss: 0.031 - mean_q: 0.855 - mean_eps: 0.221\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1395    \n",
      "2019 episodes - episode_reward: 0.691 [0.127, 1.331] - loss: 0.031 - mean_q: 0.854 - mean_eps: 0.210\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1461    \n",
      "2056 episodes - episode_reward: 0.710 [0.108, 1.334] - loss: 0.032 - mean_q: 0.864 - mean_eps: 0.199\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1370    \n",
      "2033 episodes - episode_reward: 0.674 [0.126, 1.334] - loss: 0.032 - mean_q: 0.868 - mean_eps: 0.188\n",
      "\n",
      "Interval 39 (380000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 35s - reward: 0.1551    \n",
      "2102 episodes - episode_reward: 0.738 [0.122, 1.334] - loss: 0.031 - mean_q: 0.869 - mean_eps: 0.177\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1542    \n",
      "2094 episodes - episode_reward: 0.736 [0.125, 1.334] - loss: 0.030 - mean_q: 0.871 - mean_eps: 0.166\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 34s - reward: 0.1367    - \n",
      "2027 episodes - episode_reward: 0.674 [0.131, 1.348] - loss: 0.031 - mean_q: 0.873 - mean_eps: 0.155\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1491    \n",
      "2076 episodes - episode_reward: 0.718 [0.118, 1.331] - loss: 0.031 - mean_q: 0.877 - mean_eps: 0.144\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1582    \n",
      "2117 episodes - episode_reward: 0.747 [0.132, 1.334] - loss: 0.030 - mean_q: 0.884 - mean_eps: 0.133\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1598    \n",
      "2133 episodes - episode_reward: 0.749 [0.125, 1.330] - loss: 0.030 - mean_q: 0.887 - mean_eps: 0.122\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1647    \n",
      "2145 episodes - episode_reward: 0.768 [0.123, 1.334] - loss: 0.030 - mean_q: 0.884 - mean_eps: 0.111\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1665    \n",
      "2151 episodes - episode_reward: 0.774 [0.129, 1.348] - loss: 0.030 - mean_q: 0.894 - mean_eps: 0.100\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1585    \n",
      "2133 episodes - episode_reward: 0.743 [0.123, 1.313] - loss: 0.030 - mean_q: 0.900 - mean_eps: 0.089\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 35s - reward: 0.1762    - ETA\n",
      "2193 episodes - episode_reward: 0.804 [0.130, 1.348] - loss: 0.030 - mean_q: 0.891 - mean_eps: 0.078\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1811    \n",
      "2222 episodes - episode_reward: 0.815 [0.125, 1.330] - loss: 0.030 - mean_q: 0.900 - mean_eps: 0.067\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 36s - reward: 0.1839    \n",
      "done, took 1787.729 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78906e10b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIFFICULTY = 4\n",
    "env = RubiksEnv(rubiks_hamilton_dist, scrambles=DIFFICULTY, max_step=DIFFICULTY)\n",
    "\n",
    "agent.fit(env, nb_steps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T01:27:45.329761Z",
     "start_time": "2017-10-16T01:15:26.306888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 24s - reward: 0.0787    \n",
      "1599 episodes - episode_reward: 0.492 [0.140, 1.375] - loss: 0.029 - mean_q: 0.810 - mean_eps: 0.296\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 30s - reward: 0.0707    \n",
      "1569 episodes - episode_reward: 0.450 [0.138, 1.375] - loss: 0.029 - mean_q: 0.824 - mean_eps: 0.293\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 30s - reward: 0.0719    - ETA: 0\n",
      "1573 episodes - episode_reward: 0.457 [0.136, 1.375] - loss: 0.029 - mean_q: 0.812 - mean_eps: 0.288\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 30s - reward: 0.0687    \n",
      "1571 episodes - episode_reward: 0.438 [0.139, 1.374] - loss: 0.029 - mean_q: 0.811 - mean_eps: 0.283\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.0785    - ETA: 0s - \n",
      "1592 episodes - episode_reward: 0.493 [0.139, 1.375] - loss: 0.029 - mean_q: 0.790 - mean_eps: 0.278\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.0771    \n",
      "1598 episodes - episode_reward: 0.483 [0.136, 1.393] - loss: 0.029 - mean_q: 0.780 - mean_eps: 0.273\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.0694    \n",
      "1569 episodes - episode_reward: 0.442 [0.137, 1.393] - loss: 0.029 - mean_q: 0.789 - mean_eps: 0.268\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 32s - reward: 0.0775    - ETA: 0s - reward: 0.0\n",
      "1594 episodes - episode_reward: 0.486 [0.133, 1.375] - loss: 0.029 - mean_q: 0.777 - mean_eps: 0.263\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.0657    \n",
      "1547 episodes - episode_reward: 0.424 [0.138, 1.374] - loss: 0.030 - mean_q: 0.777 - mean_eps: 0.258\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.0783    \n",
      "1602 episodes - episode_reward: 0.489 [0.141, 1.379] - loss: 0.029 - mean_q: 0.765 - mean_eps: 0.253\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 33s - reward: 0.0844    \n",
      "1619 episodes - episode_reward: 0.521 [0.141, 1.371] - loss: 0.029 - mean_q: 0.762 - mean_eps: 0.248\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.0864    - ETA: 0s - reward:\n",
      "1633 episodes - episode_reward: 0.529 [0.143, 1.372] - loss: 0.029 - mean_q: 0.774 - mean_eps: 0.243\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 31s - reward: 0.0804    -\n",
      "1611 episodes - episode_reward: 0.499 [0.134, 1.374] - loss: 0.029 - mean_q: 0.774 - mean_eps: 0.238\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0839    - ETA: 0s - re\n",
      "1620 episodes - episode_reward: 0.518 [0.138, 1.375] - loss: 0.029 - mean_q: 0.777 - mean_eps: 0.233\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0762    \n",
      "1584 episodes - episode_reward: 0.481 [0.141, 1.371] - loss: 0.030 - mean_q: 0.776 - mean_eps: 0.228\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0811    \n",
      "1617 episodes - episode_reward: 0.502 [0.144, 1.393] - loss: 0.029 - mean_q: 0.766 - mean_eps: 0.223\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0888    - ETA: 0s \n",
      "1634 episodes - episode_reward: 0.543 [0.143, 1.379] - loss: 0.029 - mean_q: 0.767 - mean_eps: 0.218\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0792    \n",
      "1601 episodes - episode_reward: 0.495 [0.138, 1.371] - loss: 0.029 - mean_q: 0.783 - mean_eps: 0.213\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0860    -\n",
      "1626 episodes - episode_reward: 0.529 [0.141, 1.379] - loss: 0.029 - mean_q: 0.785 - mean_eps: 0.208\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0899    \n",
      "1649 episodes - episode_reward: 0.545 [0.143, 1.379] - loss: 0.029 - mean_q: 0.778 - mean_eps: 0.203\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0838    \n",
      "1619 episodes - episode_reward: 0.517 [0.146, 1.348] - loss: 0.029 - mean_q: 0.776 - mean_eps: 0.198\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 29s - reward: 0.0851    \n",
      "1626 episodes - episode_reward: 0.524 [0.141, 1.374] - loss: 0.029 - mean_q: 0.787 - mean_eps: 0.193\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 30s - reward: 0.0945    \n",
      "1655 episodes - episode_reward: 0.571 [0.138, 1.375] - loss: 0.029 - mean_q: 0.788 - mean_eps: 0.188\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 30s - reward: 0.0928    \n",
      "1659 episodes - episode_reward: 0.560 [0.139, 1.393] - loss: 0.029 - mean_q: 0.792 - mean_eps: 0.183\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      " 1239/10000 [==>...........................] - ETA: 29s - reward: 0.0927done, took 739.017 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78906e18d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIFFICULTY = 5\n",
    "env = RubiksEnv(rubiks_hamilton_dist, scrambles=DIFFICULTY, max_step=DIFFICULTY)\n",
    "agent.policy.value_max = 0.3\n",
    "agent.fit(env, nb_steps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T01:28:01.323241Z",
     "start_time": "2017-10-16T01:28:01.251468Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.save_weights(\"Model_Multi_Conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
